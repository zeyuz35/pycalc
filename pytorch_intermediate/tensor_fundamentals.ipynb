{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Fundamentals\n",
    "# also known as arrays\n",
    "# PyTorch tensors the same as arrays in R (and numpy arrays), but are more suited for\n",
    "# machine learning, because they can also hold a gradient\n",
    "\n",
    "# 1 dimensional tensors are also vectors\n",
    "# 2 dimensional tensors are also matrices\n",
    "# 3 dimensional tensors are also cubes, etc\n",
    "\n",
    "# preliminary\n",
    "import torch\n",
    "# math is a built in library from C\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[1.2810e-31, 8.4078e-45, 0.0000e+00, 0.0000e+00],\n",
      "        [1.6928e+22, 1.7592e+22, 2.7664e+29, 2.5763e+20],\n",
      "        [4.7465e+27, 7.1440e+31, 1.4153e-43, 0.0000e+00]])\n"
     ]
    }
   ],
   "source": [
    "# initialize an empty tensor, 3 x 4 in dimensions\n",
    "\n",
    "x = torch.empty(3, 4)\n",
    "print(type(x))\n",
    "print(x)\n",
    "\n",
    "# notes\n",
    "# by defualt, torch.Tensor is an alias for Torch.Float, i.e. 32 bit floats\n",
    "# torch.empty() only allocates memory for a tensor\n",
    "# so the values are just whatever was in memory at the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.5240, 0.5863, 0.1338],\n",
      "        [0.2952, 0.7607, 0.2487]])\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 3)\n",
    "print(zeros)\n",
    "\n",
    "ones = torch.ones(2, 3)\n",
    "print(ones)\n",
    "\n",
    "torch.manual_seed(27935248)\n",
    "# torch.rand generates random numbers from a uniform distribution on [0, 1]\n",
    "random = torch.rand(2, 3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[3.0343e+24, 3.4713e-41, 0.0000e+00],\n",
      "         [0.0000e+00, 1.4020e+18, 3.4710e-41]],\n",
      "\n",
      "        [[1.0842e-19, 0.0000e+00, 1.0000e+00],\n",
      "         [1.0000e+00, 1.0000e+00, 1.0000e+00]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[3.0343e+24, 3.4713e-41, 0.0000e+00],\n",
      "         [0.0000e+00, 1.4020e+18, 3.4710e-41]],\n",
      "\n",
      "        [[1.0842e-19, 0.0000e+00, 2.7096e-09],\n",
      "         [4.2195e-08, 1.0606e-08, 1.2470e+16]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.3260, 0.7191, 0.1391],\n",
      "         [0.8715, 0.0733, 0.9410]],\n",
      "\n",
      "        [[0.9588, 0.9555, 0.6474],\n",
      "         [0.5305, 0.0480, 0.3496]]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor shapes\n",
    "\n",
    "x = torch.empty(2, 2, 3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "# torch.empty_like creates a tensor with the same shape/dims as the input tensor\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n",
      "tensor([[0.2145, 0.3257, 0.0873, 0.9181],\n",
      "        [0.6234, 0.2417, 0.8330, 0.7578]])\n",
      "tensor([[0.4290, 0.6514, 0.1747, 1.8362],\n",
      "        [1.2468, 0.4835, 1.6660, 1.5157]])\n"
     ]
    }
   ],
   "source": [
    "# Maths and Logic for Tensors\n",
    "# scalar addition\n",
    "ones = torch.zeros(2, 2) + 1\n",
    "# multiplication\n",
    "twos = torch.ones(2, 2) * 2\n",
    "threes = (torch.ones(2, 2) * 7 - 1) / 2\n",
    "# element by element powers\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1272, 0.3320, 0.4989, 0.2165],\n",
      "        [0.0294, 0.8858, 0.8213, 0.1649]])\n",
      "tensor([[0.2545, 0.6640, 0.9979, 0.4330],\n",
      "        [0.0589, 1.7717, 1.6426, 0.3298]])\n",
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n",
      "tensor([[[0.5772, 0.7683],\n",
      "         [0.9139, 0.2316],\n",
      "         [0.4643, 0.9097]],\n",
      "\n",
      "        [[0.5772, 0.7683],\n",
      "         [0.9139, 0.2316],\n",
      "         [0.4643, 0.9097]],\n",
      "\n",
      "        [[0.5772, 0.7683],\n",
      "         [0.9139, 0.2316],\n",
      "         [0.4643, 0.9097]],\n",
      "\n",
      "        [[0.5772, 0.7683],\n",
      "         [0.9139, 0.2316],\n",
      "         [0.4643, 0.9097]]])\n",
      "tensor([[[0.1157, 0.1157],\n",
      "         [0.8757, 0.8757],\n",
      "         [0.0276, 0.0276]],\n",
      "\n",
      "        [[0.1157, 0.1157],\n",
      "         [0.8757, 0.8757],\n",
      "         [0.0276, 0.0276]],\n",
      "\n",
      "        [[0.1157, 0.1157],\n",
      "         [0.8757, 0.8757],\n",
      "         [0.0276, 0.0276]],\n",
      "\n",
      "        [[0.1157, 0.1157],\n",
      "         [0.8757, 0.8757],\n",
      "         [0.0276, 0.0276]]])\n",
      "tensor([[[0.5020, 0.5477],\n",
      "         [0.5020, 0.5477],\n",
      "         [0.5020, 0.5477]],\n",
      "\n",
      "        [[0.5020, 0.5477],\n",
      "         [0.5020, 0.5477],\n",
      "         [0.5020, 0.5477]],\n",
      "\n",
      "        [[0.5020, 0.5477],\n",
      "         [0.5020, 0.5477],\n",
      "         [0.5020, 0.5477]],\n",
      "\n",
      "        [[0.5020, 0.5477],\n",
      "         [0.5020, 0.5477],\n",
      "         [0.5020, 0.5477]]])\n"
     ]
    }
   ],
   "source": [
    "# Tensor Broadcasting\n",
    "# Broadcasting is a way to perform operations on tensors of different shapes\n",
    "# the smaller tensor is broadcasted to the larger tensor's shape\n",
    "# Rules (going from last to first)\n",
    "# Each dimension must be equal (just conformable multiplication, boring) OR\n",
    "# One of the dimensions must be 1, OR\n",
    "# The dimension does not exist in one of the tensors\n",
    "\n",
    "rand = torch.rand(2, 4)\n",
    "doubled = rand * (torch.ones(1, 4) * 2)\n",
    "\n",
    "# \"multiplying\" a 2 x 4 tensors with a 1 x 4\n",
    "\n",
    "print(rand)\n",
    "print(doubled)\n",
    "\n",
    "a =     torch.ones(4, 3, 2)\n",
    "print(a)\n",
    "# 3rd & 2nd dims identical to a, dim 1 absent\n",
    "b = a * torch.rand(   3, 2) \n",
    "print(b)\n",
    "\n",
    "# 3rd dim = 1, 2nd dim identical to a\n",
    "# operation is broadcast over every layer and row of a, s.t. every column is identical\n",
    "c = a * torch.rand(   3, 1) \n",
    "print(c)\n",
    "\n",
    "# 3rd dim identical to a, 2nd dim = 1\n",
    "# operation is broadcast over every layer and col of a, s.t. every row is identical\n",
    "d = a * torch.rand(   1, 2) \n",
    "print(d)\n",
    "\n",
    "# These operations will fail\n",
    "# a =     torch.ones(4, 3, 2)\n",
    "# \n",
    "# b = a * torch.rand(4, 3)    # dimensions must match last-to-first\n",
    "# \n",
    "# c = a * torch.rand(   2, 3) # both 3rd & 2nd dims different\n",
    "# \n",
    "# d = a * torch.rand((0, ))   # can't broadcast with an empty tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "a: tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "# There are many more math operations available in PyTorch\n",
    "# https://pytorch.org/docs/stable/torch.html#math-operations\n",
    "\n",
    "# Modifying a tensor in place\n",
    "# The _ suffix denotes an in-place operation\n",
    "# This is more efficient than creating a new tensor\n",
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print(f\"a: {a}\")\n",
    "# note the underscore\n",
    "torch.sin_(a)\n",
    "# now a has changed\n",
    "print(f\"a: {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 999.],\n",
      "        [  1.,   1.]])\n",
      "tensor([[  1., 999.],\n",
      "        [  1.,   1.]])\n",
      "tensor([[  1., 999.],\n",
      "        [  1.,   1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Shallow vs Deep Copying\n",
    "# As is the usual case in Python, assignment only creates a new reference to the same object\n",
    "# this has roots in efficient memory management\n",
    "a = torch.ones(2, 2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 999\n",
    "\n",
    "print(a)\n",
    "# b is changed as well\n",
    "print(b)\n",
    "\n",
    "# to get around this, we can force a deep copy via the clone method\n",
    "a = torch.ones(2, 2)\n",
    "b = a.clone()\n",
    "a[0][1] = 999\n",
    "print(a)\n",
    "print(b)\n",
    "\n",
    "# IMPORTANT\n",
    "# clone also copies over the gradient, if the tensor has one.\n",
    "# this is typically the desired behaviour, but if not, you can use the .detach() method to \n",
    "# remove any gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "tensor([[0.5767, 0.0558],\n",
      "        [0.8559, 0.9901]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Managing CPU/GPU resources\n",
    "# PyTorch tensors can be moved to the GPU for faster computation\n",
    "torch.accelerator.current_accelerator()\n",
    "\n",
    "device = torch.accelerator.current_accelerator() if torch.accelerator.is_available() else torch.device('cpu')\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "x = torch.rand(2, 2, device=device)\n",
    "print(x)\n",
    "\n",
    "# very annoyingly, by default ALL tensors are created on the CPU\n",
    "# unfortunately, there doesn't seem to be an elegant way to get around this\n",
    "# so you will have to be clever and remember to move tensors to the GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
