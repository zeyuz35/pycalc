{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model:\n",
      "TinyModel(\n",
      "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=200, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Building Models with PyTorch\n",
    "\n",
    "# preliminary\n",
    "\n",
    "import torch\n",
    "\n",
    "# on a more specific level, models are subclasses of torch.nn.Module\n",
    "# derived classes may override the methods of the parent class, but otherwise\n",
    "# inherit the parent's methods and attributes\n",
    "\n",
    "class TinyModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(100, 200)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(200, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "tinymodel = TinyModel()\n",
    "\n",
    "print(\"The Model:\")\n",
    "print(tinymodel)\n",
    "\n",
    "# In what follows, we go through some really popular layer types and how to use them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "tensor([[0.9263, 0.8603, 0.3848]])\n",
      "Parameter containing:\n",
      "tensor([[-0.0094,  0.2088, -0.3791],\n",
      "        [-0.4903, -0.3000, -0.4874]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0507, -0.0682], requires_grad=True)\n",
      "tensor([[ 0.0757, -0.9681]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Common Layer Types\n",
    "\n",
    "# Linear Layer\n",
    "# This is a dense, fully connected layer by default\n",
    "\n",
    "lin = torch.nn.Linear(3, 2)\n",
    "x = torch.rand(1, 3)\n",
    "print(\"Input:\")\n",
    "print(x)\n",
    "\n",
    "for param in lin.parameters():\n",
    "    print(param)\n",
    "\n",
    "y = lin(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layers\n",
    "# These essentially \"slide\" a filter over the input tensor (image) to produce \n",
    "# a feature map that is in lower in dimension\n",
    "# The specific choice of filter depends on hyperparameters such as kernel size, stride, padding, etc.\n",
    "\n",
    "# kernel size: the size of the filter\n",
    "# stride: the number of pixels the filter moves each time\n",
    "# padding: the number of pixels added to the input tensor to ensure the output tensor is the same size\n",
    "\n",
    "# Easiest to visualize and understand convolutions with image data, but the same principles\n",
    "# also apply to other data where the spatial relationships between elements are important\n",
    "# e.g. text data\n",
    "# I suppose time series data could also count as a \"spatial\" relationship, but one would\n",
    "# need to be careful with specifying a one-sided convolution\n",
    "# similar problem to generalised dynamic factor models\n",
    "\n",
    "import torch.functional as F\n",
    "\n",
    "# LeNet example\n",
    "# Input: 1 x 32 x 32 black and white image\n",
    "# Output: 10 classes\n",
    "\n",
    "# First input to convolutional layer is the number of input channels\n",
    "# in black and white case, it is 1. For colour images, it is 3 (RGB)\n",
    "# Second input is the number of output channels. This is the number of filters\n",
    "# applied to the input image, and the number of features we ask it to produce\n",
    "# Third argument is the kernel/window size\n",
    "# Note that it is possible to specify a height different to width by passing a tuple\n",
    "# although this is somewhat strange (square filters are the norm)\n",
    "\n",
    "# A convolutional layer will produce an activation map\n",
    "# which is a 3d tensor of dimension \n",
    "# (number of output channels, height, width)\n",
    "# The height and width are determined by the input image size, kernel size, stride, and padding\n",
    "\n",
    "# In this case, the FIRST layer outputs 6 x 28 x 28, where the 28 is determined by the fact that\n",
    "# when scanning a 5 pixel window over a 32 pixel row/col, there are only 32 - 5 = 28 valid \n",
    "# positions\n",
    "\n",
    "# This is then passed through a relu activation function\n",
    "# and then through a max pooling layer with a 2 x 2 window, which merges each \n",
    "# 2 x 2 windows into a single cell\n",
    "# This results in a 6 x 14 x 14 output, where the 14 is determined by 28 / 2 = 14\n",
    "\n",
    "# The second layer expects 6 input channels due to the output of the first layer,\n",
    "# has 16 output channels, and a 3 x 3 kernel\n",
    "# This results in a \n",
    "# 16 x 12 x 12 output, where the 12 is determined by the fact that when scanning a 3 pixel window\n",
    "# over a 28 pixel row/col, there are only 28 - 3 = 12 valid positions\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 1 input image channel (black and white), 6 output channels, \n",
    "        # 5 x 5 square convolution kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, kernel_size = 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, kernel_size = 3)\n",
    "        # affine operation\n",
    "        # 6 x 6 from image dimension\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "        # forward pass\n",
    "        def forward(self, x):\n",
    "            # Max pooling over a (2, 2) window\n",
    "            x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "            # If the size is a square, you can specify with a single number\n",
    "            x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "            x = x.view(-1, self.num_flat_features(x))\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            return x\n",
    "        \n",
    "        def num_flat_features(self, x):\n",
    "            # i.e. call all dimensions except the batch dimension\n",
    "            size = x.size()[1:]\n",
    "            # initilise a variable to store the number of features\n",
    "            num_features = 1\n",
    "            for s in size:\n",
    "                num_features *= s\n",
    "            return num_features\n",
    "\n",
    "# I think this documentation is actually wrong, embarassing..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent Layers\n",
    "# These are used for sequential data, where the order of the data matters\n",
    "# e.g. time series, text, etc.\n",
    "\n",
    "# Variants include\n",
    "# vanilla RNN, LSTMs, GRU\n",
    "\n",
    "class LSTMagger(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(10, 20, 2)\n",
    "        self.linear = torch.nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # initial hidden state\n",
    "        h0 = torch.zeros(2, x.size(1), 20)\n",
    "        # initial cell state\n",
    "        c0 = torch.zeros(2, x.size(1), 20)\n",
    "        # forward pass\n",
    "        x, _ = self.lstm(x, (h0, c0))\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
