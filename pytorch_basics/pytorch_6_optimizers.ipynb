{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Optimizing Model Parameters\n",
    "# Putting this all together, and using the optimizer to actually estimate the model\n",
    "\n",
    "# preliminary\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# load in the data (FashionMNIST)\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# set up DataLoader\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# can check size of the data like so\n",
    "# training_data.data.shape\n",
    "\n",
    "# Define the model class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    # initialisation layer\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            # this is input layer, and follows because the data is 28x28\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    # forward pass method\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# create the model, and move it to the optimal device\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "\n",
    "# Set up the hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# Initialise the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "# very vanilla stochastic gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop main ingredients\n",
    "# call optimizer.zero_grad() to reset the gradients first\n",
    "# calculate the gradients via backprop using loss.backward()\n",
    "# call optimizer step to \"nudge\" the parameters in the right direction using those gradients\n",
    "# repeat\n",
    "\n",
    "# PyTorch wants you to define your own train loop function, then then manually loop through it\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # first set the model in training mode, this is important for batch normalization\n",
    "    # and dropout layers. \n",
    "    # Actually unnecessary in this specific model (which doesn't have either), but\n",
    "    # added for best practice\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Move tensors to appropraite device first\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # if there are no more batches, then print the loss\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch*batch_size + len(X)\n",
    "            # print the loss to 5 decimal places\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # set the model to evaluation mode, important for batch normalization and dropout\n",
    "    # layers.\n",
    "    # Technically unnecessary in this specific model, but added for best practice\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    # initialize empty variables to store the loss and the number of correct predictions\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluate the model with torch.no_grad() to prevent gradient tracking \n",
    "    # (i.e. gradients accumulating when computing the forward pass)\n",
    "    # how does this with statement work?\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            # Move tensors to appropraite device first\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.381158  [   64/60000]\n",
      "loss: 1.330277  [ 6464/60000]\n",
      "loss: 1.098648  [12864/60000]\n",
      "loss: 1.112260  [19264/60000]\n",
      "loss: 1.165929  [25664/60000]\n",
      "loss: 1.223262  [32064/60000]\n",
      "loss: 1.240569  [38464/60000]\n",
      "loss: 1.333045  [44864/60000]\n",
      "loss: 1.255942  [51264/60000]\n",
      "loss: 1.145032  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Avg loss: 1.210869 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.348202  [   64/60000]\n",
      "loss: 1.307628  [ 6464/60000]\n",
      "loss: 1.066619  [12864/60000]\n",
      "loss: 1.083502  [19264/60000]\n",
      "loss: 1.145869  [25664/60000]\n",
      "loss: 1.196983  [32064/60000]\n",
      "loss: 1.220352  [38464/60000]\n",
      "loss: 1.314123  [44864/60000]\n",
      "loss: 1.229030  [51264/60000]\n",
      "loss: 1.127525  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Avg loss: 1.189526 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.318947  [   64/60000]\n",
      "loss: 1.288324  [ 6464/60000]\n",
      "loss: 1.039439  [12864/60000]\n",
      "loss: 1.058799  [19264/60000]\n",
      "loss: 1.129309  [25664/60000]\n",
      "loss: 1.174480  [32064/60000]\n",
      "loss: 1.202806  [38464/60000]\n",
      "loss: 1.297863  [44864/60000]\n",
      "loss: 1.205394  [51264/60000]\n",
      "loss: 1.112300  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Avg loss: 1.170870 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.292733  [   64/60000]\n",
      "loss: 1.271249  [ 6464/60000]\n",
      "loss: 1.016053  [12864/60000]\n",
      "loss: 1.037412  [19264/60000]\n",
      "loss: 1.115521  [25664/60000]\n",
      "loss: 1.155034  [32064/60000]\n",
      "loss: 1.187440  [38464/60000]\n",
      "loss: 1.283985  [44864/60000]\n",
      "loss: 1.184420  [51264/60000]\n",
      "loss: 1.098938  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 1.154291 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.268813  [   64/60000]\n",
      "loss: 1.255736  [ 6464/60000]\n",
      "loss: 0.995593  [12864/60000]\n",
      "loss: 1.018929  [19264/60000]\n",
      "loss: 1.103037  [25664/60000]\n",
      "loss: 1.138085  [32064/60000]\n",
      "loss: 1.173659  [38464/60000]\n",
      "loss: 1.271464  [44864/60000]\n",
      "loss: 1.165857  [51264/60000]\n",
      "loss: 1.087368  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.8%, Avg loss: 1.139393 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.247135  [   64/60000]\n",
      "loss: 1.241541  [ 6464/60000]\n",
      "loss: 0.977778  [12864/60000]\n",
      "loss: 1.002605  [19264/60000]\n",
      "loss: 1.091936  [25664/60000]\n",
      "loss: 1.123235  [32064/60000]\n",
      "loss: 1.160014  [38464/60000]\n",
      "loss: 1.260245  [44864/60000]\n",
      "loss: 1.149408  [51264/60000]\n",
      "loss: 1.077039  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.5%, Avg loss: 1.125897 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.227569  [   64/60000]\n",
      "loss: 1.228273  [ 6464/60000]\n",
      "loss: 0.962332  [12864/60000]\n",
      "loss: 0.987908  [19264/60000]\n",
      "loss: 1.082124  [25664/60000]\n",
      "loss: 1.109707  [32064/60000]\n",
      "loss: 1.147253  [38464/60000]\n",
      "loss: 1.250120  [44864/60000]\n",
      "loss: 1.135050  [51264/60000]\n",
      "loss: 1.067710  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.2%, Avg loss: 1.113615 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.210116  [   64/60000]\n",
      "loss: 1.216213  [ 6464/60000]\n",
      "loss: 0.948796  [12864/60000]\n",
      "loss: 0.974796  [19264/60000]\n",
      "loss: 1.072714  [25664/60000]\n",
      "loss: 1.097955  [32064/60000]\n",
      "loss: 1.135253  [38464/60000]\n",
      "loss: 1.241376  [44864/60000]\n",
      "loss: 1.122577  [51264/60000]\n",
      "loss: 1.059347  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.7%, Avg loss: 1.102399 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.194851  [   64/60000]\n",
      "loss: 1.205064  [ 6464/60000]\n",
      "loss: 0.937069  [12864/60000]\n",
      "loss: 0.962950  [19264/60000]\n",
      "loss: 1.064347  [25664/60000]\n",
      "loss: 1.087251  [32064/60000]\n",
      "loss: 1.124088  [38464/60000]\n",
      "loss: 1.233730  [44864/60000]\n",
      "loss: 1.111889  [51264/60000]\n",
      "loss: 1.051530  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.1%, Avg loss: 1.092204 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.181500  [   64/60000]\n",
      "loss: 1.194762  [ 6464/60000]\n",
      "loss: 0.926790  [12864/60000]\n",
      "loss: 0.952381  [19264/60000]\n",
      "loss: 1.056963  [25664/60000]\n",
      "loss: 1.077790  [32064/60000]\n",
      "loss: 1.113824  [38464/60000]\n",
      "loss: 1.226903  [44864/60000]\n",
      "loss: 1.102734  [51264/60000]\n",
      "loss: 1.044432  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.6%, Avg loss: 1.082949 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Begin training!\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
